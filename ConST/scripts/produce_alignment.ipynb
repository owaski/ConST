{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install praat-textgrids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from string import punctuation\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import transformers\n",
    "import sentencepiece\n",
    "import torchaudio\n",
    "import textgrids\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import pandas as pd\n",
    "\n",
    "from g2p_en import G2p\n",
    "\n",
    "from fairseq.data import PhonemeDictionary\n",
    "from ConST.prepare_data.data_utils import load_df_from_tsv, save_df_to_tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/mnt/data/siqiouyang/datasets/must-c-v1.0/'\n",
    "lang = 'es'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "# spm = sentencepiece.SentencePieceProcessor(os.path.join(root, 'spm_unigram10000_st_{}.model'.format(lang)))\n",
    "# spm = sentencepiece.SentencePieceProcessor(os.path.join(root, 'flores200sacrebleuspm.model'))\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    '/mnt/taurus/data/xixu/llm/llama-2-7b/hf',\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_df_from_tsv(os.path.join(root, '{}_st_{}.tsv'.format(split, lang)))\n",
    "# df = load_df_from_tsv(os.path.join(root, '{}_st_{}.tsv'.format(split)))\n",
    "# df = load_df_from_tsv(os.path.join(root, 'train_asr_mt_iwslt.tsv'))\n",
    "# df2 = load_df_from_tsv(os.path.join(root, 'dev_asr_mt_iwslt.tsv'))\n",
    "# df3 = load_df_from_tsv(os.path.join(root, 'train_asr_mt_cv.tsv'))\n",
    "# df4 = load_df_from_tsv(os.path.join(root, 'dev_asr_mt_cv.tsv'))\n",
    "# df2 = load_df_from_tsv(os.path.join(root, 'dev_st_id_en.tsv'))\n",
    "# df3 = load_df_from_tsv(os.path.join(root, 'test_st_id_en.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat([df, df2, df3, df4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>audio</th>\n",
       "      <th>n_frames</th>\n",
       "      <th>speaker</th>\n",
       "      <th>src_text</th>\n",
       "      <th>tgt_text</th>\n",
       "      <th>src_lang</th>\n",
       "      <th>tgt_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ted_1_0</td>\n",
       "      <td>en-es/data/train/wav/ted_1.wav:1739840:130720</td>\n",
       "      <td>130720</td>\n",
       "      <td>spk.1</td>\n",
       "      <td>(Laughter) I looked in the rear-view mirror an...</td>\n",
       "      <td>Sé que suena como cualquier cosa para ustedes,...</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ted_1_1</td>\n",
       "      <td>en-es/data/train/wav/ted_1.wav:1872800:99679</td>\n",
       "      <td>99679</td>\n",
       "      <td>spk.1</td>\n",
       "      <td>There was no motorcade back there. (Laughter) ...</td>\n",
       "      <td>No había caravana de vehículos allá atrás. ¿Ha...</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ted_1_2</td>\n",
       "      <td>en-es/data/train/wav/ted_1.wav:2163679:74880</td>\n",
       "      <td>74880</td>\n",
       "      <td>spk.1</td>\n",
       "      <td>(Laughter) It was dinnertime, and we started l...</td>\n",
       "      <td>Era la hora de la cena, y comenzamos a buscar ...</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ted_1_3</td>\n",
       "      <td>en-es/data/train/wav/ted_1.wav:2254400:8800</td>\n",
       "      <td>8800</td>\n",
       "      <td>spk.1</td>\n",
       "      <td>We were on I-40.</td>\n",
       "      <td>Estábamos en la I-40.</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ted_1_4</td>\n",
       "      <td>en-es/data/train/wav/ted_1.wav:2271040:51840</td>\n",
       "      <td>51840</td>\n",
       "      <td>spk.1</td>\n",
       "      <td>We got to Exit 238, Lebanon, Tennessee.</td>\n",
       "      <td>Llegamos a la Salida 238, Líbano, Tennessee.</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260036</th>\n",
       "      <td>ted_13591_40</td>\n",
       "      <td>en-es/data/train/wav/ted_13591.wav:851680:126240</td>\n",
       "      <td>126240</td>\n",
       "      <td>spk.13591</td>\n",
       "      <td>Then I would beat my plate with a spoon and my...</td>\n",
       "      <td>Luego golpeaba mi plato con una cuchara y me b...</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260037</th>\n",
       "      <td>ted_13591_41</td>\n",
       "      <td>en-es/data/train/wav/ted_13591.wav:988800:172160</td>\n",
       "      <td>172160</td>\n",
       "      <td>spk.13591</td>\n",
       "      <td>I have been beating the same plates, shakers, ...</td>\n",
       "      <td>Desde entonces sigo golpeando los mismos plato...</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260038</th>\n",
       "      <td>ted_13591_43</td>\n",
       "      <td>en-es/data/train/wav/ted_13591.wav:1263840:96160</td>\n",
       "      <td>96160</td>\n",
       "      <td>spk.13591</td>\n",
       "      <td>As I grew up, subconsciously, I felt a strong ...</td>\n",
       "      <td>A medida que iba creciendo, sentía inconscient...</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260039</th>\n",
       "      <td>ted_13591_44</td>\n",
       "      <td>en-es/data/train/wav/ted_13591.wav:1363680:91520</td>\n",
       "      <td>91520</td>\n",
       "      <td>spk.13591</td>\n",
       "      <td>Even without it being said out loud, I knew th...</td>\n",
       "      <td>Y aunque no lo decía en voz alta, sabía que es...</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260040</th>\n",
       "      <td>ted_13591_45</td>\n",
       "      <td>en-es/data/train/wav/ted_13591.wav:1482560:295040</td>\n",
       "      <td>295040</td>\n",
       "      <td>spk.13591</td>\n",
       "      <td>In most of the ceremonies, I noticed that most...</td>\n",
       "      <td>En la mayoría de las ceremonias, notaba que ca...</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260041 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                              audio  \\\n",
       "0            ted_1_0      en-es/data/train/wav/ted_1.wav:1739840:130720   \n",
       "1            ted_1_1       en-es/data/train/wav/ted_1.wav:1872800:99679   \n",
       "2            ted_1_2       en-es/data/train/wav/ted_1.wav:2163679:74880   \n",
       "3            ted_1_3        en-es/data/train/wav/ted_1.wav:2254400:8800   \n",
       "4            ted_1_4       en-es/data/train/wav/ted_1.wav:2271040:51840   \n",
       "...              ...                                                ...   \n",
       "260036  ted_13591_40   en-es/data/train/wav/ted_13591.wav:851680:126240   \n",
       "260037  ted_13591_41   en-es/data/train/wav/ted_13591.wav:988800:172160   \n",
       "260038  ted_13591_43   en-es/data/train/wav/ted_13591.wav:1263840:96160   \n",
       "260039  ted_13591_44   en-es/data/train/wav/ted_13591.wav:1363680:91520   \n",
       "260040  ted_13591_45  en-es/data/train/wav/ted_13591.wav:1482560:295040   \n",
       "\n",
       "        n_frames    speaker  \\\n",
       "0         130720      spk.1   \n",
       "1          99679      spk.1   \n",
       "2          74880      spk.1   \n",
       "3           8800      spk.1   \n",
       "4          51840      spk.1   \n",
       "...          ...        ...   \n",
       "260036    126240  spk.13591   \n",
       "260037    172160  spk.13591   \n",
       "260038     96160  spk.13591   \n",
       "260039     91520  spk.13591   \n",
       "260040    295040  spk.13591   \n",
       "\n",
       "                                                 src_text  \\\n",
       "0       (Laughter) I looked in the rear-view mirror an...   \n",
       "1       There was no motorcade back there. (Laughter) ...   \n",
       "2       (Laughter) It was dinnertime, and we started l...   \n",
       "3                                        We were on I-40.   \n",
       "4                 We got to Exit 238, Lebanon, Tennessee.   \n",
       "...                                                   ...   \n",
       "260036  Then I would beat my plate with a spoon and my...   \n",
       "260037  I have been beating the same plates, shakers, ...   \n",
       "260038  As I grew up, subconsciously, I felt a strong ...   \n",
       "260039  Even without it being said out loud, I knew th...   \n",
       "260040  In most of the ceremonies, I noticed that most...   \n",
       "\n",
       "                                                 tgt_text src_lang tgt_lang  \n",
       "0       Sé que suena como cualquier cosa para ustedes,...       en       es  \n",
       "1       No había caravana de vehículos allá atrás. ¿Ha...       en       es  \n",
       "2       Era la hora de la cena, y comenzamos a buscar ...       en       es  \n",
       "3                                   Estábamos en la I-40.       en       es  \n",
       "4            Llegamos a la Salida 238, Líbano, Tennessee.       en       es  \n",
       "...                                                   ...      ...      ...  \n",
       "260036  Luego golpeaba mi plato con una cuchara y me b...       en       es  \n",
       "260037  Desde entonces sigo golpeando los mismos plato...       en       es  \n",
       "260038  A medida que iba creciendo, sentía inconscient...       en       es  \n",
       "260039  Y aunque no lo decía en voz alta, sabía que es...       en       es  \n",
       "260040  En la mayoría de las ceremonias, notaba que ca...       en       es  \n",
       "\n",
       "[260041 rows x 8 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479.9419181770833"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['n_frames'].sum() / 16000 / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_frames = 0\n",
    "# for fn in os.listdir('/mnt/data/siqiouyang/datasets/must-c-v1.0/en-de/data/dev/wav'):\n",
    "#     info = torchaudio.info('/mnt/data/siqiouyang/datasets/must-c-v1.0/en-de/data/dev/wav/' + fn)\n",
    "#     n_frames += info.num_frames\n",
    "# n_frames / 16000 / 3600\n",
    "\n",
    "# import yaml\n",
    "# with open('/mnt/data/siqiouyang/datasets/must-c-v1.0/en-de/data/train/txt/train.yaml') as r:\n",
    "#     y = yaml.load(r, Loader=yaml.Loader)\n",
    "\n",
    "# duration = 0\n",
    "# for x in y:\n",
    "#     duration += x['duration']\n",
    "# duration / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = list(range(train_df.shape[0]))\n",
    "# random.shuffle(indices)\n",
    "# save_df_to_tsv(train_df.iloc[indices[:10000]], os.path.join(root, 'train-tiny_asr.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(root, 'en-{}'.format(lang), 'data', split, 'align_sllama')\n",
    "# save_dir = os.path.join(root, 'mt-en', 'data', 'asr', 'align_mfat')\n",
    "# save_dir = os.path.join(root, 'data', split, 'align_sllama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/data/siqiouyang/datasets/must-c-v1.0/en-es/data/train/align_sllama'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1312/1312 [00:03<00:00, 414.78it/s]\n"
     ]
    }
   ],
   "source": [
    "last_audio_path = None\n",
    "for idx in tqdm(range(len(df))):\n",
    "    audio_path, offset, num_frames = os.path.join(root, df['audio'][idx]).split(':')\n",
    "    # audio_path = os.path.join(root, df['audio'][idx])\n",
    "    # offset, num_frames = 0, df['n_frames'][idx]\n",
    "    offset, num_frames = int(offset), int(num_frames)\n",
    "    if last_audio_path is None or audio_path != last_audio_path:\n",
    "        waveform, frame_rate = torchaudio.load(os.path.join(root, audio_path))\n",
    "        last_audio_path = audio_path\n",
    "    torchaudio.save(os.path.join(save_dir, '{}.wav'.format(df['id'][idx])), waveform[:, offset : offset + num_frames], sample_rate=frame_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['src_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuation = '!\"#$%&,.?' # for maltese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 318/260041 [00:00<01:21, 3173.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260041/260041 [01:08<00:00, 3788.12it/s]\n"
     ]
    }
   ],
   "source": [
    "def covered(s, punctuation):\n",
    "    for c in s:\n",
    "        if c not in punctuation:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "space = '▁'\n",
    "tokenized_sentences = []\n",
    "segmentss = []\n",
    "punctuation = punctuation\n",
    "for sent in tqdm(df['src_text'].tolist()):\n",
    "    # tokens = spm.EncodeAsPieces(sent)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(sent, add_special_tokens=False))\n",
    "    segments = []\n",
    "    last = -1\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if token.startswith(space) or covered(token, punctuation):\n",
    "            if last != -1 and last <= idx - 1:\n",
    "                segments.append((last, idx - 1))\n",
    "            last = idx + (token == space or covered(token, punctuation) or \\\n",
    "                (token.startswith(space) and len(token) > 1 and covered(token[1:], punctuation)))    \n",
    "    \n",
    "    if last < len(tokens):\n",
    "        segments.append((last, len(tokens) - 1))\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    for seg in segments:\n",
    "        token = ''.join(tokens[seg[0] : seg[1] + 1]).replace(space, '')\n",
    "        if token.replace(',', '').isnumeric():\n",
    "            token = token.replace(',', '')\n",
    "        tokenized_sentence.append(token)\n",
    "\n",
    "    tokenized_sentences.append(tokenized_sentence)\n",
    "    segmentss.append(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1312 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/data/siqiouyang/datasets/must-c-v1.0/en-es/data/dev/align_sllama/ted_767/ted_767_0.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     write_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;28mid\u001b[39m[:\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m.\u001b[39mrfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m w:\n\u001b[1;32m      7\u001b[0m     w\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokenized_sentences[i]))\n",
      "File \u001b[0;32m~/anaconda3/envs/ConST/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/siqiouyang/datasets/must-c-v1.0/en-es/data/dev/align_sllama/ted_767/ted_767_0.txt'"
     ]
    }
   ],
   "source": [
    "for i, id in enumerate(tqdm(df['id'])):\n",
    "    # if id.startswith('common_voice'):\n",
    "    #     write_dir = os.path.join(save_dir, df.iloc[i]['speaker'])\n",
    "    # else:\n",
    "    #     write_dir = os.path.join(save_dir, id[:id.rfind('_')])\n",
    "    write_dir = save_dir \n",
    "    with open(os.path.join(write_dir, '{}.txt'.format(id)), 'w') as w:\n",
    "        w.write(' '.join(tokenized_sentences[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "mfa align --clean . english_mfa english_mfa textgrids \n",
    "mfa train --clean -o model/acoustic_model --phone_set IPA --output_format long_textgrid --include_original_text -t /mnt/data/siqiouyang/cache/MFA/mfat_enes ./ english_mfa ./textgrids \n",
    "\n",
    "mfa train --clean -o model/acoustic_model --phone_set IPA --output_format long_textgrid --include_original_text -t /mnt/data/siqiouyang/cache/MFA/mfat_id ./ indonesian_cv ./textgrids --clean\n",
    "mfa align . indonesian_cv model/acoustic_model textgrids\n",
    "\n",
    "mfa train --clean -o model/acoustic_model --phone_set IPA --output_format long_textgrid --include_original_text -t /mnt/data/siqiouyang/cache/MFA/mfat_mt ./ maltese_cv ./textgrids -j 36\n",
    "mfa align --clean . maltese_cv model/acoustic_model.zip textgrids \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260041/260041 [07:24<00:00, 584.73it/s] \n"
     ]
    }
   ],
   "source": [
    "filtered_grids = []\n",
    "n_outlier = 0\n",
    "sum_duration = 0.\n",
    "ids = []\n",
    "speech_word_col = []\n",
    "text_word_col = []\n",
    "for i, id in enumerate(tqdm(df['id'])):\n",
    "\n",
    "    # subdir = df.iloc[i]['speaker'] if id.startswith('common_voice') else id[:id.rfind('_')]\n",
    "\n",
    "    grid_path = os.path.join(save_dir, 'textgrids/{}.TextGrid'.format(id))\n",
    "\n",
    "    if os.path.exists(grid_path):\n",
    "        ids.append(id)\n",
    "        grid = textgrids.TextGrid(grid_path)\n",
    "        filtered_grid = [tok for tok in grid['words'] if tok.text != '']\n",
    "\n",
    "        if len(filtered_grid) != len(tokenized_sentences[i]):\n",
    "            # print(i, [w.text for w in filtered_grid], tokenized_sentences[i], sep='\\n')\n",
    "            n_outlier += 1\n",
    "            speech_word_col.append(None)\n",
    "            text_word_col.append(None)\n",
    "            continue\n",
    "\n",
    "        speech_word = [(word.xmin, word.xmax) for word in filtered_grid]\n",
    "        text_word = segmentss[i]\n",
    "\n",
    "        speech_word_col.append(speech_word)\n",
    "        text_word_col.append(text_word)\n",
    "\n",
    "        # audio_path = os.path.join(save_dir, '{}.wav'.format(id))\n",
    "        # info = torchaudio.info(audio_path)\n",
    "        # duration = info.num_frames / info.sample_rate\n",
    "        # interval = interval / duration\n",
    "\n",
    "        # sum_duration += duration\n",
    "\n",
    "        # print(id, tokenized_sentences[i])\n",
    "\n",
    "        # th.save([segmentss[i], interval], os.path.join(save_dir, '{}.pt'.format(id)))\n",
    "    else:\n",
    "        speech_word_col.append(None)\n",
    "        text_word_col.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['speech_word'] = speech_word_col\n",
    "df['text_word'] = text_word_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>audio</th>\n",
       "      <th>n_frames</th>\n",
       "      <th>speaker</th>\n",
       "      <th>src_text</th>\n",
       "      <th>tgt_text</th>\n",
       "      <th>src_lang</th>\n",
       "      <th>tgt_lang</th>\n",
       "      <th>speech_word</th>\n",
       "      <th>text_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ted_1_0</td>\n",
       "      <td>en-es/data/train/wav/ted_1.wav:1739840:130720</td>\n",
       "      <td>130720</td>\n",
       "      <td>spk.1</td>\n",
       "      <td>(Laughter) I looked in the rear-view mirror an...</td>\n",
       "      <td>Sé que suena como cualquier cosa para ustedes,...</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "      <td>[(0.0, 0.27), (1.7, 1.89), (1.89, 2.17), (2.17...</td>\n",
       "      <td>[(1, 3), (5, 5), (6, 6), (7, 7), (8, 8), (9, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ted_1_1</td>\n",
       "      <td>en-es/data/train/wav/ted_1.wav:1872800:99679</td>\n",
       "      <td>99679</td>\n",
       "      <td>spk.1</td>\n",
       "      <td>There was no motorcade back there. (Laughter) ...</td>\n",
       "      <td>No había caravana de vehículos allá atrás. ¿Ha...</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "      <td>[(0.0, 0.13), (0.13, 0.31), (0.31, 0.47), (0.4...</td>\n",
       "      <td>[(0, 0), (1, 1), (2, 2), (3, 4), (5, 5), (6, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ted_1_2</td>\n",
       "      <td>en-es/data/train/wav/ted_1.wav:2163679:74880</td>\n",
       "      <td>74880</td>\n",
       "      <td>spk.1</td>\n",
       "      <td>(Laughter) It was dinnertime, and we started l...</td>\n",
       "      <td>Era la hora de la cena, y comenzamos a buscar ...</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "      <td>[(0.0, 0.27), (0.27, 0.37), (0.37, 0.71), (0.7...</td>\n",
       "      <td>[(1, 3), (5, 5), (6, 6), (7, 10), (12, 12), (1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ted_1_3</td>\n",
       "      <td>en-es/data/train/wav/ted_1.wav:2254400:8800</td>\n",
       "      <td>8800</td>\n",
       "      <td>spk.1</td>\n",
       "      <td>We were on I-40.</td>\n",
       "      <td>Estábamos en la I-40.</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "      <td>[(0.0, 0.06), (0.06, 0.22), (0.22, 0.33), (0.3...</td>\n",
       "      <td>[(0, 0), (1, 1), (2, 2), (3, 3), (5, 6)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ted_1_4</td>\n",
       "      <td>en-es/data/train/wav/ted_1.wav:2271040:51840</td>\n",
       "      <td>51840</td>\n",
       "      <td>spk.1</td>\n",
       "      <td>We got to Exit 238, Lebanon, Tennessee.</td>\n",
       "      <td>Llegamos a la Salida 238, Líbano, Tennessee.</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "      <td>[(0.0, 0.04), (0.04, 0.31), (0.31, 1.09), (1.1...</td>\n",
       "      <td>[(0, 0), (1, 1), (2, 2), (3, 3), (5, 7), (9, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260036</th>\n",
       "      <td>ted_13591_40</td>\n",
       "      <td>en-es/data/train/wav/ted_13591.wav:851680:126240</td>\n",
       "      <td>126240</td>\n",
       "      <td>spk.13591</td>\n",
       "      <td>Then I would beat my plate with a spoon and my...</td>\n",
       "      <td>Luego golpeaba mi plato con una cuchara y me b...</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "      <td>[(0.0, 0.63), (1.57, 1.7), (1.7, 1.98), (1.98,...</td>\n",
       "      <td>[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260037</th>\n",
       "      <td>ted_13591_41</td>\n",
       "      <td>en-es/data/train/wav/ted_13591.wav:988800:172160</td>\n",
       "      <td>172160</td>\n",
       "      <td>spk.13591</td>\n",
       "      <td>I have been beating the same plates, shakers, ...</td>\n",
       "      <td>Desde entonces sigo golpeando los mismos plato...</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "      <td>[(0.03, 0.15), (0.15, 0.38), (0.38, 0.55), (0....</td>\n",
       "      <td>[(0, 0), (1, 1), (2, 2), (3, 4), (5, 5), (6, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260038</th>\n",
       "      <td>ted_13591_43</td>\n",
       "      <td>en-es/data/train/wav/ted_13591.wav:1263840:96160</td>\n",
       "      <td>96160</td>\n",
       "      <td>spk.13591</td>\n",
       "      <td>As I grew up, subconsciously, I felt a strong ...</td>\n",
       "      <td>A medida que iba creciendo, sentía inconscient...</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "      <td>[(0.03, 0.15), (0.15, 0.47), (0.5, 0.72), (0.7...</td>\n",
       "      <td>[(0, 0), (1, 1), (2, 2), (3, 3), (5, 8), (10, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260039</th>\n",
       "      <td>ted_13591_44</td>\n",
       "      <td>en-es/data/train/wav/ted_13591.wav:1363680:91520</td>\n",
       "      <td>91520</td>\n",
       "      <td>spk.13591</td>\n",
       "      <td>Even without it being said out loud, I knew th...</td>\n",
       "      <td>Y aunque no lo decía en voz alta, sabía que es...</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "      <td>[(0.03, 0.3), (0.3, 0.63), (0.63, 0.78), (0.78...</td>\n",
       "      <td>[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260040</th>\n",
       "      <td>ted_13591_45</td>\n",
       "      <td>en-es/data/train/wav/ted_13591.wav:1482560:295040</td>\n",
       "      <td>295040</td>\n",
       "      <td>spk.13591</td>\n",
       "      <td>In most of the ceremonies, I noticed that most...</td>\n",
       "      <td>En la mayoría de las ceremonias, notaba que ca...</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "      <td>[(0.03, 0.15), (0.15, 0.42), (0.42, 0.51), (0....</td>\n",
       "      <td>[(0, 0), (1, 1), (2, 2), (3, 3), (4, 6), (8, 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260041 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                              audio  \\\n",
       "0            ted_1_0      en-es/data/train/wav/ted_1.wav:1739840:130720   \n",
       "1            ted_1_1       en-es/data/train/wav/ted_1.wav:1872800:99679   \n",
       "2            ted_1_2       en-es/data/train/wav/ted_1.wav:2163679:74880   \n",
       "3            ted_1_3        en-es/data/train/wav/ted_1.wav:2254400:8800   \n",
       "4            ted_1_4       en-es/data/train/wav/ted_1.wav:2271040:51840   \n",
       "...              ...                                                ...   \n",
       "260036  ted_13591_40   en-es/data/train/wav/ted_13591.wav:851680:126240   \n",
       "260037  ted_13591_41   en-es/data/train/wav/ted_13591.wav:988800:172160   \n",
       "260038  ted_13591_43   en-es/data/train/wav/ted_13591.wav:1263840:96160   \n",
       "260039  ted_13591_44   en-es/data/train/wav/ted_13591.wav:1363680:91520   \n",
       "260040  ted_13591_45  en-es/data/train/wav/ted_13591.wav:1482560:295040   \n",
       "\n",
       "        n_frames    speaker  \\\n",
       "0         130720      spk.1   \n",
       "1          99679      spk.1   \n",
       "2          74880      spk.1   \n",
       "3           8800      spk.1   \n",
       "4          51840      spk.1   \n",
       "...          ...        ...   \n",
       "260036    126240  spk.13591   \n",
       "260037    172160  spk.13591   \n",
       "260038     96160  spk.13591   \n",
       "260039     91520  spk.13591   \n",
       "260040    295040  spk.13591   \n",
       "\n",
       "                                                 src_text  \\\n",
       "0       (Laughter) I looked in the rear-view mirror an...   \n",
       "1       There was no motorcade back there. (Laughter) ...   \n",
       "2       (Laughter) It was dinnertime, and we started l...   \n",
       "3                                        We were on I-40.   \n",
       "4                 We got to Exit 238, Lebanon, Tennessee.   \n",
       "...                                                   ...   \n",
       "260036  Then I would beat my plate with a spoon and my...   \n",
       "260037  I have been beating the same plates, shakers, ...   \n",
       "260038  As I grew up, subconsciously, I felt a strong ...   \n",
       "260039  Even without it being said out loud, I knew th...   \n",
       "260040  In most of the ceremonies, I noticed that most...   \n",
       "\n",
       "                                                 tgt_text src_lang tgt_lang  \\\n",
       "0       Sé que suena como cualquier cosa para ustedes,...       en       es   \n",
       "1       No había caravana de vehículos allá atrás. ¿Ha...       en       es   \n",
       "2       Era la hora de la cena, y comenzamos a buscar ...       en       es   \n",
       "3                                   Estábamos en la I-40.       en       es   \n",
       "4            Llegamos a la Salida 238, Líbano, Tennessee.       en       es   \n",
       "...                                                   ...      ...      ...   \n",
       "260036  Luego golpeaba mi plato con una cuchara y me b...       en       es   \n",
       "260037  Desde entonces sigo golpeando los mismos plato...       en       es   \n",
       "260038  A medida que iba creciendo, sentía inconscient...       en       es   \n",
       "260039  Y aunque no lo decía en voz alta, sabía que es...       en       es   \n",
       "260040  En la mayoría de las ceremonias, notaba que ca...       en       es   \n",
       "\n",
       "                                              speech_word  \\\n",
       "0       [(0.0, 0.27), (1.7, 1.89), (1.89, 2.17), (2.17...   \n",
       "1       [(0.0, 0.13), (0.13, 0.31), (0.31, 0.47), (0.4...   \n",
       "2       [(0.0, 0.27), (0.27, 0.37), (0.37, 0.71), (0.7...   \n",
       "3       [(0.0, 0.06), (0.06, 0.22), (0.22, 0.33), (0.3...   \n",
       "4       [(0.0, 0.04), (0.04, 0.31), (0.31, 1.09), (1.1...   \n",
       "...                                                   ...   \n",
       "260036  [(0.0, 0.63), (1.57, 1.7), (1.7, 1.98), (1.98,...   \n",
       "260037  [(0.03, 0.15), (0.15, 0.38), (0.38, 0.55), (0....   \n",
       "260038  [(0.03, 0.15), (0.15, 0.47), (0.5, 0.72), (0.7...   \n",
       "260039  [(0.03, 0.3), (0.3, 0.63), (0.63, 0.78), (0.78...   \n",
       "260040  [(0.03, 0.15), (0.15, 0.42), (0.42, 0.51), (0....   \n",
       "\n",
       "                                                text_word  \n",
       "0       [(1, 3), (5, 5), (6, 6), (7, 7), (8, 8), (9, 9...  \n",
       "1       [(0, 0), (1, 1), (2, 2), (3, 4), (5, 5), (6, 6...  \n",
       "2       [(1, 3), (5, 5), (6, 6), (7, 10), (12, 12), (1...  \n",
       "3                [(0, 0), (1, 1), (2, 2), (3, 3), (5, 6)]  \n",
       "4       [(0, 0), (1, 1), (2, 2), (3, 3), (5, 7), (9, 1...  \n",
       "...                                                   ...  \n",
       "260036  [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5...  \n",
       "260037  [(0, 0), (1, 1), (2, 2), (3, 4), (5, 5), (6, 6...  \n",
       "260038  [(0, 0), (1, 1), (2, 2), (3, 3), (5, 8), (10, ...  \n",
       "260039  [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5...  \n",
       "260040  [(0, 0), (1, 1), (2, 2), (3, 3), (4, 6), (8, 8...  \n",
       "\n",
       "[260041 rows x 10 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_to_tsv(df, os.path.join(root, '{}_st_{}_mfa.tsv'.format(split, lang)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.324904479166667"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['n_frames'].sum() / 16000 / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.026304687499981"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_duration / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1889837289943985"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_outlier / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8703 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jiena jogħġbuni wkoll. ['Jiena', 'jogħġbuni', 'wkoll'] [(0, 1), (2, 3), (4, 4)] [[0.         0.0304878 ]\n",
      " [0.0304878  0.95121951]\n",
      " [0.95121951 0.98780488]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_grids = []\n",
    "n_outlier = 0\n",
    "for i, id in enumerate(tqdm(df['id'])):\n",
    "    grid_path = os.path.join(save_dir, 'textgrids/{}.TextGrid'.format(id))\n",
    "    if os.path.exists(grid_path):\n",
    "        grid = textgrids.TextGrid(grid_path)\n",
    "        filtered_grid = [tok for tok in grid['words'] if tok.text != '']\n",
    "\n",
    "        if len(filtered_grid) != len(tokenized_sentences[i]):\n",
    "            # print(i, [w.text for w in filtered_grid], tokenized_sentences[i], sep='\\n')\n",
    "            n_outlier += 1\n",
    "            continue\n",
    "\n",
    "        interval = np.array([(word.xmin, word.xmax) for word in filtered_grid])\n",
    "        audio_path = os.path.join(save_dir, '{}.wav'.format(id))\n",
    "        info = torchaudio.info(audio_path)\n",
    "        duration = info.num_frames / info.sample_rate\n",
    "        interval = interval / duration\n",
    "\n",
    "        os.system('cp {} ./'.format(os.path.join(save_dir, '{}.wav'.format(id))))\n",
    "        print(sentences[i], tokenized_sentences[i], segmentss[i], interval)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_grids = []\n",
    "n_outlier = 0\n",
    "for i, id in enumerate(tqdm(df['id'])):\n",
    "    grid_path = os.path.join(save_dir, 'textgrids/{}.TextGrid'.format(id))\n",
    "    if os.path.exists(grid_path):\n",
    "        grid = textgrids.TextGrid(grid_path)\n",
    "\n",
    "        phones = [phone.text if phone.text != '' else '<empty>' for phone in grid['phones']]\n",
    "\n",
    "        interval = np.array([(phone.xmin, phone.xmax) for phone in grid['phones']])\n",
    "        audio_path = os.path.join(save_dir, '{}.wav'.format(id))\n",
    "        info = torchaudio.info(audio_path)\n",
    "        duration = info.num_frames / info.sample_rate\n",
    "        interval = interval / duration\n",
    "\n",
    "        th.save([segmentss[i], interval], os.path.join(save_dir, '{}.phone.pt'.format(id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/data/siqiouyang/datasets/must-c-v1.0/phone.txt', 'r') as r:\n",
    "    all_phones = [p.strip() for p in r.readlines() if p.strip() != '']\n",
    "    phone_dict = {p : i for i, p in enumerate(all_phones)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2p = G2p()\n",
    "src_dict = PhonemeDictionary.load(os.path.join(root, 'phonemes.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_ls960_asr.tsv: 100%|██████████| 281241/281241 [10:31<00:00, 445.27it/s]\n",
      "dev_ls960_asr.tsv: 100%|██████████| 5567/5567 [00:07<00:00, 721.19it/s]\n"
     ]
    }
   ],
   "source": [
    "for fn in os.listdir(root):\n",
    "    # if fn.endswith('tsv') and ('asr' in fn or 'de.' in fn):\n",
    "    if 'ls' in fn:\n",
    "\n",
    "        df = load_df_from_tsv(os.path.join(root, fn))\n",
    "        list_of_phonemes = []\n",
    "        for src_text in tqdm(df['src_text'], desc=fn):\n",
    "            raw_phonemes = g2p(src_text)\n",
    "            phonemes = []\n",
    "            for idx in range(len(raw_phonemes)):\n",
    "                if raw_phonemes[idx] in src_dict:\n",
    "                    p = raw_phonemes[idx]\n",
    "                    if idx == 0 or raw_phonemes[idx - 1] not in src_dict:\n",
    "                        p = '▁' + p\n",
    "                    phonemes.append(p)\n",
    "            list_of_phonemes.append(' '.join(phonemes))\n",
    "        df['src_phoneme'] = list_of_phonemes\n",
    "        save_df_to_tsv(df, os.path.join(root, fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5567/5567 [00:00<00:00, 18686.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# For Librispeech\n",
    "\n",
    "df = load_df_from_tsv('/mnt/data/siqiouyang/datasets/must-c-v1.0/dev_ls960_asr.tsv')\n",
    "\n",
    "def covered(s, punctuation):\n",
    "    for c in s:\n",
    "        if c not in punctuation:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "space = '▁'\n",
    "tokenized_sentences = []\n",
    "segmentss = []\n",
    "punctuation = punctuation + '—’'\n",
    "punctuation = punctuation.replace(\"'\", '')\n",
    "for sent in tqdm(df['src_text'].tolist()):\n",
    "    tokens = spm.EncodeAsPieces(sent)\n",
    "    segments = []\n",
    "    last = -1\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if token.startswith(space) or covered(token, punctuation):\n",
    "            if last != -1 and last <= idx - 1:\n",
    "                segments.append((last, idx - 1))\n",
    "            last = idx + (token == space or covered(token, punctuation) or \\\n",
    "                (token.startswith(space) and len(token) > 1 and covered(token[1:], punctuation)))    \n",
    "    \n",
    "    if last < len(tokens):\n",
    "        segments.append((last, len(tokens) - 1))\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    for seg in segments:\n",
    "        token = ''.join(tokens[seg[0] : seg[1] + 1]).replace(space, '')\n",
    "        if token.replace(',', '').isnumeric():\n",
    "            token = token.replace(',', '')\n",
    "        tokenized_sentence.append(token)\n",
    "\n",
    "    tokenized_sentences.append(tokenized_sentence)\n",
    "    segmentss.append(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00: 100%|██████████| 5567/5567 [00:06<00:00, 895.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0014370396982216634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_grids = []\n",
    "n_outlier = 0\n",
    "iterator = tqdm(df['id'])\n",
    "for i, id in enumerate(iterator):\n",
    "    grid_path = os.path.join('/mnt/data/siqiouyang/datasets/librispeech/LibriSpeech/librispeech_mfa/textgrids/{}'.format(df['speaker'][i]), '{}.TextGrid'.format(id))\n",
    "    if os.path.exists(grid_path):\n",
    "        grid = textgrids.TextGrid(grid_path)\n",
    "        filtered_grid = [tok for tok in grid['words'] if tok.text != '']\n",
    "\n",
    "        u = v = 0\n",
    "        intervals = []\n",
    "        fail = False\n",
    "        while u < len(filtered_grid) and v < len(tokenized_sentences[i]):\n",
    "            if filtered_grid[u].text == tokenized_sentences[i][v]:\n",
    "                intervals.append((filtered_grid[u].xmin, filtered_grid[u].xmax))\n",
    "                u += 1\n",
    "                v += 1\n",
    "            elif tokenized_sentences[i][v].startswith(filtered_grid[u].text):\n",
    "                if u < len(filtered_grid) - 1 and tokenized_sentences[i][v] == filtered_grid[u].text + filtered_grid[u + 1].text:\n",
    "                    intervals.append((filtered_grid[u].xmin, filtered_grid[u + 1].xmax))\n",
    "                    u += 2\n",
    "                    v += 1\n",
    "                else:\n",
    "                    fail = True\n",
    "                    break\n",
    "            else:\n",
    "                fail = True\n",
    "                break\n",
    "        \n",
    "        if u < len(filtered_grid) or v < len(tokenized_sentences[i]):\n",
    "            fail = True\n",
    "\n",
    "        iterator.set_description('{:.2f}'.format(n_outlier / (i + 1)))\n",
    "\n",
    "        if fail:\n",
    "            # print(i, [w.text for w in filtered_grid], tokenized_sentences[i], sep='\\n')\n",
    "            # break\n",
    "            n_outlier += 1\n",
    "            continue\n",
    "            \n",
    "\n",
    "        interval = np.array(intervals)\n",
    "        \n",
    "        audio_path = os.path.join('/mnt/data/siqiouyang/datasets/librispeech', df['audio'][i])\n",
    "        info = torchaudio.info(audio_path)\n",
    "        duration = info.num_frames / info.sample_rate\n",
    "        interval = interval / duration\n",
    "\n",
    "        assert len(segmentss[i]) == len(interval)\n",
    "        th.save([segmentss[i], interval], os.path.join('/mnt/data/siqiouyang/datasets/librispeech/LibriSpeech/librispeech_mfa/{}'.format(df['speaker'][i]), '{}.pt'.format(id)))\n",
    "print(n_outlier / len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03123891, 0.05679801],\n",
       "       [0.05679801, 0.07880724],\n",
       "       [0.10365637, 0.12992545],\n",
       "       [0.12992545, 0.15264466],\n",
       "       [0.15264466, 0.17607384],\n",
       "       [0.18033369, 0.19027334],\n",
       "       [0.19027334, 0.23358182],\n",
       "       [0.3542776 , 0.38125666],\n",
       "       [0.38125666, 0.40397586],\n",
       "       [0.40397586, 0.43237487],\n",
       "       [0.44373447, 0.46077387],\n",
       "       [0.46645367, 0.49272275],\n",
       "       [0.49272275, 0.50479233],\n",
       "       [0.50479233, 0.51828186],\n",
       "       [0.51828186, 0.53319134],\n",
       "       [0.53319134, 0.58146965],\n",
       "       [0.58146965, 0.60773873],\n",
       "       [0.62193823, 0.63968761],\n",
       "       [0.64820731, 0.66950657],\n",
       "       [0.66950657, 0.68512602],\n",
       "       [0.68512602, 0.68938587],\n",
       "       [0.68938587, 0.70642528],\n",
       "       [0.70642528, 0.73837416],\n",
       "       [0.76038339, 0.79233227],\n",
       "       [0.79233227, 0.80085197],\n",
       "       [0.80085197, 0.8342208 ],\n",
       "       [0.8342208 , 0.8455804 ],\n",
       "       [0.8455804 , 0.87042953],\n",
       "       [0.87042953, 0.90876819],\n",
       "       [0.90876819, 0.91444799],\n",
       "       [0.91444799, 0.9485268 ],\n",
       "       [0.9485268 , 0.9570465 ],\n",
       "       [0.9570465 , 0.96343628],\n",
       "       [0.96343628, 0.98970536]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Librispeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/mnt/data/siqiouyang/datasets/must-c-v1.0'\n",
    "train_splits = [\"train-clean-100\", \"train-clean-360\", \"train-other-500\"]\n",
    "dev_splits = [\"dev-clean\", \"dev-other\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for split in train_splits:\n",
    "    df = load_df_from_tsv(os.path.join(root, split + '.tsv'))\n",
    "    dfs.append(df)\n",
    "combined_df = pd.concat(dfs)\n",
    "save_df_to_tsv(combined_df, os.path.join(root, 'train_ls960_asr.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for split in dev_splits:\n",
    "    df = load_df_from_tsv(os.path.join(root, split + '.tsv'))\n",
    "    dfs.append(df)\n",
    "combined_df = pd.concat(dfs)\n",
    "save_df_to_tsv(combined_df, os.path.join(root, 'dev_ls960_asr.tsv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce Low Resource TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_df_from_tsv(os.path.join(root, '{}_st_{}.tsv'.format(split, lang)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for duration in [0]: # in hours\n",
    "    limit = duration * 60 * 60 * 16000\n",
    "    indices = list(range(df.shape[0]))\n",
    "    sel_mask = np.zeros((df.shape[0]), dtype=bool)\n",
    "    if duration > 0:\n",
    "        random.shuffle(indices)\n",
    "        total = 0\n",
    "        for i, idx in enumerate(indices):\n",
    "            total += df['n_frames'][idx]\n",
    "            sel_mask[idx] = True\n",
    "            if total > limit:\n",
    "                break\n",
    "    st_df = df.iloc[sel_mask]\n",
    "    asr_df = df.iloc[~sel_mask]\n",
    "\n",
    "    # filter those without .pt files\n",
    "    filter_mask = np.zeros((asr_df.shape[0]), dtype=bool)\n",
    "    for i, id in enumerate(asr_df['id']):\n",
    "        pt_path = os.path.join(root, 'en-{}'.format(lang), 'data/{}/align'.format(split), '{}.pt'.format(id))\n",
    "        if not os.path.exists(pt_path):\n",
    "            filter_mask[i] = True\n",
    "    asr_df = asr_df.iloc[~filter_mask]\n",
    "\n",
    "    if duration > 0:\n",
    "        save_df_to_tsv(st_df, os.path.join(root, 'train-{}h_st.tsv'.format(duration)))\n",
    "    save_df_to_tsv(asr_df, os.path.join(root, 'train-{}h_asr.tsv'.format(duration)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225271, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215748, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_df_from_tsv(os.path.join(root, 'train-1h_asr.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 100 * 60 * 60 * 16000\n",
    "indices = list(range(df.shape[0]))\n",
    "sel_mask = np.zeros((df.shape[0]), dtype=bool)\n",
    "random.shuffle(indices)\n",
    "total = 0\n",
    "for i, idx in enumerate(indices):\n",
    "    total += df['n_frames'][idx]\n",
    "    sel_mask[idx] = True\n",
    "    if total > limit:\n",
    "        break\n",
    "asr_df = df.iloc[sel_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.00037677083333"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_df['n_frames'].sum() / 16000 / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_to_tsv(asr_df, os.path.join(root, 'train-1h_asr_100h.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ConST')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b19e2bae1ea557e2a235ed68e1ca6fc95eb26397d1b9313344955976d03228b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
