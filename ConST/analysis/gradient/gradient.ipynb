{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from argparse import Namespace\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import sacrebleu\n",
    "import sentencepiece\n",
    "\n",
    "from fairseq import utils\n",
    "from fairseq.data import Dictionary, data_utils as fairseq_data_utils\n",
    "from fairseq.models.speech_to_text.xstnet import XSTNet\n",
    "from fairseq.data.audio.speech_text_triple_align_dataset import (\n",
    "    SpeechTextTripleAlignDataset\n",
    ")\n",
    "from fairseq.criterions.multitask_crossentropy_with_contrastive_token_with_extra_mt import MultiTaskCrossEntropyWithContrastiveTokenWithExtraMT\n",
    "from fairseq.data.audio.speech_to_text_dataset import get_features_or_waveform, _collate_frames\n",
    "from ConST.prepare_data.data_utils import load_df_from_tsv, save_df_to_tsv\n",
    "from fairseq.checkpoint_utils import load_checkpoint_to_cpu, save_state, torch_persistent_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "task = Namespace()\n",
    "\n",
    "args.w2v2_model_path = '/mnt/data/siqiouyang/runs/mST/pretrained/wav2vec_small.pt'\n",
    "\n",
    "args.max_audio_positions = 600000\n",
    "args.max_source_positions = 1024\n",
    "args.max_target_positions = 1024\n",
    "args.max_audio_tokens = 1000000\n",
    "args.max_text_tokens = 2000\n",
    "args.max_tokens = 1000000\n",
    "args.max_tokens_valid = 2000000\n",
    "\n",
    "tgt_dict = Dictionary.load('/mnt/data/siqiouyang/datasets/must-c-v1.0/spm_unigram10000_st_de.txt')\n",
    "task.target_dictionary = tgt_dict\n",
    "\n",
    "model = XSTNet.build_model(args, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'main_ende_token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = '/mnt/data/siqiouyang/runs/ConST/{}/checkpoint_best.pt'.format(tag)\n",
    "ckpt = load_checkpoint_to_cpu(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(ckpt['model'])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm = sentencepiece.SentencePieceProcessor()\n",
    "spm.Load('/mnt/data/siqiouyang/datasets/must-c-v1.0/spm_unigram10000_st_de.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_df_from_tsv('/mnt/data/siqiouyang/datasets/must-c-v1.0/tst-COMMON_st_de.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = MultiTaskCrossEntropyWithContrastiveTokenWithExtraMT(task, False, 0., 1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_gradient(model):\n",
    "    all_grad = []\n",
    "    for param in model.encoder.parameters():\n",
    "        if param.requires_grad and param.grad is not None:\n",
    "            all_grad.append(param.grad.flatten())\n",
    "    all_grad = th.cat(all_grad, dim=0)\n",
    "    return all_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sim(idx):\n",
    "    align_path = '/mnt/data/siqiouyang/datasets/must-c-v1.0/en-de/data/tst-COMMON/align/{}.pt'.format(df['id'][idx])\n",
    "    if os.path.exists(align_path):\n",
    "        audio_path = os.path.join('/mnt/data/siqiouyang/datasets/must-c-v1.0', df['audio'][idx])\n",
    "        audio = get_features_or_waveform(audio_path, need_waveform=True).to(device)\n",
    "        \n",
    "        tokenized =  \" \".join(spm.EncodeAsPieces(df['src_text'][idx]))\n",
    "        src_text = tgt_dict.encode_line(\n",
    "            tokenized, add_if_not_exist=False, append_eos=True\n",
    "        ).long()\n",
    "        lang_tag = SpeechTextTripleAlignDataset.LANG_TAG_TEMPLATE.format(df['src_lang'][idx])\n",
    "        lang_tag_idx = tgt_dict.index(lang_tag)\n",
    "        src_text = th.cat((th.LongTensor([lang_tag_idx]), src_text), 0).unsqueeze(0).to(device)\n",
    "\n",
    "        n_frame = th.LongTensor([audio.size(1)]).to(device)\n",
    "        src_length = th.LongTensor([src_text.size(1)])\n",
    "\n",
    "        tokenized = \" \".join(spm.EncodeAsPieces(df['tgt_text'][idx]))\n",
    "        tgt_text = tgt_dict.encode_line(\n",
    "            tokenized, add_if_not_exist=False, append_eos=True\n",
    "        ).long()\n",
    "        lang_tag = SpeechTextTripleAlignDataset.LANG_TAG_TEMPLATE.format(df['tgt_lang'][idx])\n",
    "        lang_tag_idx = tgt_dict.index(lang_tag)\n",
    "        tgt_text = th.cat((th.LongTensor([lang_tag_idx]), tgt_text), 0)\n",
    "\n",
    "        target = fairseq_data_utils.collate_tokens(\n",
    "            [tgt_text],\n",
    "            tgt_dict.pad(),\n",
    "            tgt_dict.eos(),\n",
    "            left_pad=False,\n",
    "            move_eos_to_beginning=False,\n",
    "        ).to(device)\n",
    "        prev_output_target_tokens = fairseq_data_utils.collate_tokens(\n",
    "            [tgt_text],\n",
    "            tgt_dict.pad(),\n",
    "            tgt_dict.eos(),\n",
    "            left_pad=False,\n",
    "            move_eos_to_beginning=True,\n",
    "        ).to(device)\n",
    "\n",
    "        sample = {\n",
    "            \"net_input\": {\n",
    "                \"src_tokens\": audio,\n",
    "                \"src_lengths\": n_frame,\n",
    "                \"prev_output_tokens\": prev_output_target_tokens\n",
    "            },\n",
    "            \"target\": target\n",
    "        }\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss, nll_loss, _ = loss_fn.compute_loss_st(model, sample, reduce=True)\n",
    "        nll_loss.backward()\n",
    "\n",
    "        st_grad = gather_gradient(model)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        st_encoder_out = model.encoder(audio.to(device), n_frame.to(device))\n",
    "        st_x = st_encoder_out.encoder_out.squeeze(1)\n",
    "        mt_encoder_out = model.encoder(src_text.to(device), src_length.to(device), is_text_input=True)\n",
    "        mt_x = mt_encoder_out.encoder_out.squeeze(1)\n",
    "\n",
    "        seg, itv = th.load(align_path)\n",
    "        seg = [(s[0] + 1, s[1] + 1) for s in seg]\n",
    "        itv = (itv * st_encoder_out.encoder_padding_mask.size(1)).astype(int)\n",
    "        st_f = []\n",
    "        mt_f = []\n",
    "        for (t_l, t_r), (s_l, s_r) in zip(seg, itv):\n",
    "            st_f.append(st_x[s_l : s_r + 1].mean(dim=0))\n",
    "            mt_f.append(mt_x[t_l : t_r + 1].mean(dim=0))\n",
    "        st_f = th.stack(st_f, dim=0)\n",
    "        mt_f = th.stack(mt_f, dim=0)\n",
    "\n",
    "        logits = F.cosine_similarity(\n",
    "            st_f.unsqueeze(1),\n",
    "            mt_f.unsqueeze(0),\n",
    "            dim=-1\n",
    "        ) / 0.05\n",
    "\n",
    "        label = th.arange(st_f.size(0)).to(logits.device)\n",
    "        loss = F.cross_entropy(logits, label, reduction='sum')\n",
    "        loss.backward()\n",
    "\n",
    "        token_cst_grad = gather_gradient(model)\n",
    "        token_cos_sim = (st_grad * token_cst_grad).sum() / st_grad.norm() / token_cst_grad.norm()\n",
    "\n",
    "        # model.zero_grad()\n",
    "\n",
    "        # st_encoder_out = model.encoder(audio.to(device), n_frame.to(device))\n",
    "        # st_x = st_encoder_out.encoder_out\n",
    "        # mt_encoder_out = model.encoder(src_text.to(device), src_length.to(device), is_text_input=True)\n",
    "        # mt_x = mt_encoder_out.encoder_out\n",
    "\n",
    "        # st_encoder_padding_mask = st_encoder_out.encoder_padding_mask\n",
    "        # st_encoder_out = st_x.transpose(0, 1) # T x B x hid -> B x T x hid\n",
    "        # st_encoder_padding_mask = (~st_encoder_padding_mask).float()\n",
    "        # st_seq_hidden = (st_encoder_out * st_encoder_padding_mask.unsqueeze(-1)).sum(dim=1) / st_encoder_padding_mask.sum(dim=1).unsqueeze(-1)\n",
    "\n",
    "        # mt_encoder_padding_mask = mt_encoder_out.encoder_padding_mask\n",
    "        # mt_encoder_out = mt_x.transpose(0, 1) # T x B x hid -> B x T x hid\n",
    "        # mt_encoder_padding_mask = (~mt_encoder_padding_mask).float()\n",
    "        # mt_seq_hidden = (mt_encoder_out * mt_encoder_padding_mask.unsqueeze(-1)).sum(dim=1) / mt_encoder_padding_mask.sum(dim=1).unsqueeze(-1)\n",
    "\n",
    "        # logits = F.cosine_similarity(st_seq_hidden.expand((1, 1, 512)), mt_seq_hidden.expand((1, 1, 512)).transpose(0, 1), dim=-1) / 0.02\n",
    "        # loss = -th.nn.LogSoftmax(0)(logits).diag()\n",
    "        # loss.backward()\n",
    "\n",
    "        # sent_cst_grad = gather_gradient(model)\n",
    "        # sent_cos_sim = (st_grad * sent_cst_grad).sum() / st_grad.norm() / sent_cst_grad.norm()\n",
    "        return token_cos_sim\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d045a9e3965e48be886a176975ab2fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2587 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_cos_sims = []\n",
    "for idx in tqdm(range(len(df))):\n",
    "    token_cos_sims.append(compute_sim(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_cos_sims = [s.item() for s in token_cos_sims if s is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7ff461d7ca30>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(token_cos_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ConST')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b19e2bae1ea557e2a235ed68e1ca6fc95eb26397d1b9313344955976d03228b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
