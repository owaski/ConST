{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from argparse import Namespace\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import sacrebleu\n",
    "import sentencepiece\n",
    "\n",
    "from fairseq import utils\n",
    "from fairseq.data import Dictionary, data_utils as fairseq_data_utils\n",
    "from fairseq.models.speech_to_text.xstnet import XSTNet\n",
    "from fairseq.data.audio.speech_text_triple_align_dataset import (\n",
    "    SpeechTextTripleAlignDataset\n",
    ")\n",
    "from fairseq.data.audio.speech_to_text_dataset import get_features_or_waveform, _collate_frames\n",
    "from ConST.prepare_data.data_utils import load_df_from_tsv, save_df_to_tsv\n",
    "from fairseq.checkpoint_utils import load_checkpoint_to_cpu, save_state, torch_persistent_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_per_layer_per_tag = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "task = Namespace()\n",
    "\n",
    "args.w2v2_model_path = '/mnt/data/siqiouyang/runs/mST/pretrained/wav2vec_small.pt'\n",
    "\n",
    "args.max_audio_positions = 600000\n",
    "args.max_source_positions = 1024\n",
    "args.max_target_positions = 1024\n",
    "args.max_audio_tokens = 1000000\n",
    "args.max_text_tokens = 2000\n",
    "args.max_tokens = 1000000\n",
    "args.max_tokens_valid = 2000000\n",
    "\n",
    "tgt_dict = Dictionary.load('/mnt/data/siqiouyang/datasets/must-c-v1.0/spm_unigram10000_st_de.txt')\n",
    "task.target_dictionary = tgt_dict\n",
    "\n",
    "model = XSTNet.build_model(args, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'main_ende_token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XSTNet(\n",
       "  (encoder): XSTNetEncoder(\n",
       "    (embed_tokens): Embedding(10000, 512, padding_idx=1)\n",
       "    (dropout_module): FairseqDropout()\n",
       "    (wav2vec_model): Wav2Vec2Model(\n",
       "      (feature_extractor): ConvFeatureExtractionModel(\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "            (3): GELU()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): GELU()\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): GELU()\n",
       "          )\n",
       "          (3): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): GELU()\n",
       "          )\n",
       "          (4): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): GELU()\n",
       "          )\n",
       "          (5): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): GELU()\n",
       "          )\n",
       "          (6): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): GELU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout_input): Dropout(p=0.1, inplace=False)\n",
       "      (dropout_features): Dropout(p=0.1, inplace=False)\n",
       "      (quantizer): GumbelVectorQuantizer(\n",
       "        (weight_proj): Linear(in_features=512, out_features=640, bias=True)\n",
       "      )\n",
       "      (project_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (encoder): TransformerEncoder(\n",
       "        (pos_conv): Sequential(\n",
       "          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "          (1): SamePad()\n",
       "          (2): GELU()\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (final_proj): Linear(in_features=768, out_features=256, bias=True)\n",
       "    )\n",
       "    (subsample_audio): Conv1dSubsampler(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Conv1d(768, 1024, kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "        (1): Conv1d(512, 1024, kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "      )\n",
       "    )\n",
       "    (embed_positions): SinusoidalPositionalEmbedding()\n",
       "    (transformer_layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TransformerDecoderScriptable(\n",
       "    (dropout_module): FairseqDropout()\n",
       "    (embed_tokens): Embedding(10000, 512, padding_idx=1)\n",
       "    (embed_positions): SinusoidalPositionalEmbedding()\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerDecoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerDecoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerDecoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerDecoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerDecoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerDecoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (output_projection): Linear(in_features=512, out_features=10000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = '/mnt/data/siqiouyang/runs/ConST/{}/checkpoint_best.pt'.format(tag)\n",
    "ckpt = load_checkpoint_to_cpu(ckpt_path)\n",
    "model.load_state_dict(ckpt['model'])\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm = sentencepiece.SentencePieceProcessor()\n",
    "spm.Load('/mnt/data/siqiouyang/datasets/must-c-v1.0/spm_unigram10000_st_de.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_df_from_tsv('/mnt/data/siqiouyang/datasets/must-c-v1.0/tst-COMMON_st_de.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_inputs(df_idx):\n",
    "    audio_path = os.path.join('/mnt/data/siqiouyang/datasets/must-c-v1.0', df['audio'][df_idx])\n",
    "    audio = get_features_or_waveform(audio_path, need_waveform=True)\n",
    "    tokenized =  \" \".join(spm.EncodeAsPieces(df['src_text'][df_idx]))\n",
    "    src_text = tgt_dict.encode_line(\n",
    "        tokenized, add_if_not_exist=False, append_eos=True\n",
    "    ).long()\n",
    "    lang_tag = SpeechTextTripleAlignDataset.LANG_TAG_TEMPLATE.format(df['src_lang'][df_idx])\n",
    "    lang_tag_idx = tgt_dict.index(lang_tag)\n",
    "    src_text = th.cat((th.LongTensor([lang_tag_idx]), src_text), 0).unsqueeze(0)\n",
    "    n_frame = th.LongTensor([audio.size(1)])\n",
    "    src_length = th.LongTensor([src_text.size(1)])\n",
    "\n",
    "    # Target\n",
    "    tokenized = \" \".join(spm.EncodeAsPieces(df['tgt_text'][df_idx]))\n",
    "    tgt_text = tgt_dict.encode_line(\n",
    "        tokenized, add_if_not_exist=False, append_eos=True\n",
    "    ).long()\n",
    "    lang_tag = SpeechTextTripleAlignDataset.LANG_TAG_TEMPLATE.format(df['tgt_lang'][df_idx])\n",
    "    lang_tag_idx = tgt_dict.index(lang_tag)\n",
    "    tgt_text = th.cat((th.LongTensor([lang_tag_idx]), tgt_text), 0)\n",
    "\n",
    "    prev_output_target_tokens = fairseq_data_utils.collate_tokens(\n",
    "        [tgt_text],\n",
    "        tgt_dict.pad(),\n",
    "        tgt_dict.eos(),\n",
    "        left_pad=False,\n",
    "        move_eos_to_beginning=True,\n",
    "    )\n",
    "\n",
    "    return audio, n_frame, src_text, src_length, tgt_text, prev_output_target_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature(model, df_idx):\n",
    "    audio, n_frame, src_text, src_length, tgt_text, prev_output_target_tokens = obtain_inputs(df_idx)\n",
    "\n",
    "    prev_output_target_tokens = fairseq_data_utils.collate_tokens(\n",
    "        [tgt_text],\n",
    "        tgt_dict.pad(),\n",
    "        tgt_dict.eos(),\n",
    "        left_pad=False,\n",
    "        move_eos_to_beginning=True,\n",
    "    )\n",
    "\n",
    "    with th.no_grad():\n",
    "        st_out = model.encoder(audio.to(device), n_frame.to(device))\n",
    "        mt_out = model.encoder(src_text.to(device), src_length.to(device), is_text_input=True)\n",
    "    \n",
    "    return st_out, mt_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a21349b73c4a3b98bdc8d25b92ff81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2587 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sim_per_layer = [[] for _ in range(6)]\n",
    "norm_per_layer = [[] for _ in range(6)]\n",
    "\n",
    "with th.no_grad():\n",
    "\n",
    "    for idx in tqdm(range(len(df))):\n",
    "        st_out, mt_out = compute_feature(model, idx)\n",
    "        x = th.rand(10, 1, 512).to(device)\n",
    "\n",
    "        for l in range(6):\n",
    "            st_cross, _ = model.decoder.layers[l].encoder_attn(\n",
    "                query=x,\n",
    "                key=st_out.encoder_out,\n",
    "                value=st_out.encoder_out,\n",
    "                key_padding_mask=st_out.encoder_padding_mask,\n",
    "                incremental_state=None,\n",
    "                static_kv=True,\n",
    "                need_weights=False,\n",
    "                need_head_weights=False,\n",
    "            )\n",
    "            mt_cross, _ = model.decoder.layers[l].encoder_attn(\n",
    "                query=x,\n",
    "                key=mt_out.encoder_out,\n",
    "                value=mt_out.encoder_out,\n",
    "                key_padding_mask=mt_out.encoder_padding_mask,\n",
    "                incremental_state=None,\n",
    "                static_kv=True,\n",
    "                need_weights=False,\n",
    "                need_head_weights=False,\n",
    "            )\n",
    "\n",
    "            sim = F.cosine_similarity(st_cross, mt_cross, dim=-1)\n",
    "            # sim = (st_cross - mt_cross).norm()\n",
    "\n",
    "            sim_per_layer[l].extend(sim.flatten().cpu().tolist())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_per_layer_per_tag[tag] = sim_per_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f550f11f550>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(sim_per_layer_per_tag[tag][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b7a420b290420b96583f1216cd2b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sim_per_layer = [[] for _ in range(6)]\n",
    "\n",
    "with th.no_grad():\n",
    "\n",
    "    for _ in tqdm(range(100)):\n",
    "\n",
    "        i = np.random.randint(0, len(df))\n",
    "        # j = np.random.randint(0, len(df))\n",
    "        j = i\n",
    "\n",
    "        s_i, t_i = compute_feature(model, i)\n",
    "        s_j, t_j = compute_feature(model, j)\n",
    "\n",
    "        x = th.rand(10, 1, 512).to(device)\n",
    "\n",
    "        for l in range(6):\n",
    "            st_cross_i, _ = model.decoder.layers[l].encoder_attn(\n",
    "                query=x,\n",
    "                key=s_i.encoder_out,\n",
    "                value=s_i.encoder_out,\n",
    "                key_padding_mask=s_i.encoder_padding_mask,\n",
    "                incremental_state=None,\n",
    "                static_kv=True,\n",
    "                need_weights=False,\n",
    "                need_head_weights=False,\n",
    "            )\n",
    "            mt_cross_j, _ = model.decoder.layers[l].encoder_attn(\n",
    "                query=x,\n",
    "                key=t_j.encoder_out,\n",
    "                value=t_j.encoder_out,\n",
    "                key_padding_mask=t_j.encoder_padding_mask,\n",
    "                incremental_state=None,\n",
    "                static_kv=True,\n",
    "                need_weights=False,\n",
    "                need_head_weights=False,\n",
    "            )\n",
    "\n",
    "            # sim = F.cosine_similarity(st_cross_i, mt_cross_j, dim=-1)\n",
    "            sim = (st_cross_i - mt_cross_j).norm() / mt_cross_j.norm()\n",
    "\n",
    "            sim_per_layer[l].extend(sim.flatten().cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f1c4111baf0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(sim_per_layer[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ConST')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b19e2bae1ea557e2a235ed68e1ca6fc95eb26397d1b9313344955976d03228b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
